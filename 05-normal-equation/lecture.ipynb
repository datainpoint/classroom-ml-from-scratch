{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Python 機器學習從零至一 \n",
    "\n",
    "> 正規方程\n",
    "\n",
    "[數據交點](https://www.datainpoint.com/) | 郭耀仁 <yaojenkuo@datainpoint.com>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from pyvizml import NormalEquation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 關於數值預測的任務"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## （複習）數值預測的任務：迴歸模型\n",
    "\n",
    "- 「數值預測」是「監督式學習」的其中一種應用類型。\n",
    "- 預測的目標向量 $y$ 屬於連續型數值變數。\n",
    "- 更常被稱為「迴歸模型」。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## （複習）預測 NBA 球員的體重\n",
    "\n",
    "- 資料（Experience）：一定數量的球員資料。\n",
    "- 任務（Task）：利用模型預測球員的體重。\n",
    "- 評估（Performance）：模型預測的體重與球員實際體重的誤差大小。\n",
    "- 但書（Condition）：隨著資料觀測值筆數增加，預測誤差應該要減少。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# players_stats 資料中的 weightKilograms\n",
    "csv_url = \"https://raw.githubusercontent.com/yaojenkuo/ml-newbies/master/player_stats.csv\"\n",
    "player_stats = pd.read_csv(csv_url)\n",
    "y = player_stats[\"weightKilograms\"].values\n",
    "y.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 該如何預測 NBA 球員的體重\n",
    "\n",
    "- 隨意猜測的虛假模型。\n",
    "- 基於規則（Rule-based）的專家模型。\n",
    "- 基於最小化損失函數的機器學習模型。\n",
    "- ...等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 基於最小化損失函數的機器學習模型\n",
    "\n",
    "將 `heightMeters` 當作特徵矩陣 $x_i$ 作為體重的預測依據。\n",
    "\n",
    "\\begin{equation}\n",
    "\\operatorname*{arg\\,min}_w \\; \\frac{1}{m}\\sum_{i}^{m}{(y^{(train)}_i - \\hat{y_i}^{(train)})^2} = \\frac{1}{m}\\sum_{i}^{m}{(y^{(train)}_i - x_i^{(train)} w)^2}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "X = player_stats[\"heightMeters\"].values.reshape(-1, 1)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "h = LinearRegression()\n",
    "h.fit(X_train, y_train)\n",
    "y_hat = h.predict(X_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 評估基於最小化損失函數的機器學習模型：均方誤差\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{MSE}_{valid} = \\frac{1}{m}\\sum_{i}^{m}{(y^{(valid)}_i - \\hat{y_i}^{(valid)})^2}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62.807098266825335"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse_ml = np.sum((y_valid - y_hat)**2) / y_valid.size\n",
    "mse_ml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 如何決定 `X_train` 與 `y_train` 之間的關聯 $w$\n",
    "\n",
    "Scikit-Learn 預測器的關鍵：`fit()` 方法：正規方程（Normal equation）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 正規方程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 使用基於最小化損失函數的機器學習模型解決數值預測的任務\n",
    "\n",
    "創造一個 $h$ 函數可以將無標籤資料 $x$ 作為輸入，以係數 $w$ 相乘後輸出 $\\hat{y}$\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{y} = h(x; w) = w_0 + w_1x_1 + ... + w_nx_n\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 寫作成向量矩陣相乘形式，為 $w_0$ 補上 $x_0=1$\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{y} &= w_0x_0 + w_1x_1 + ... + w_nx_n, \\; where \\; x_0 = 1 \\\\\n",
    "&= w^Tx\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 向量矩陣相乘形式\n",
    "\n",
    "- $m + 1$ 為觀測值列數。\n",
    "- $n + 1$ 是特徵個數。\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{y} = h(X; w) = \n",
    "\\begin{bmatrix} x_{00}, x_{01}, ..., x_{0n} \\\\ x_{10}, x_{11}, ..., x_{1n} \\\\.\\\\.\\\\.\\\\ x_{m0}, x_{m1}, ..., x_{mn}\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix} w_0 \\\\ w_1 \\\\.\\\\.\\\\.\\\\ w_n \\end{bmatrix} = Xw\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## $h(X; w)$ 是基於 $w$ 的函數\n",
    "\n",
    "- 如果第 $i$ 個特徵 $x_i$ 對應的係數 $w_i$ 為正數，該特徵與 $\\hat{y}$ 的變動同向。\n",
    "- 如果第 $i$ 個特徵 $x_i$ 對應的係數 $w_i$ 為負數，該特徵與 $\\hat{y}$ 的變動反向。\n",
    "- 如果第 $i$ 個特徵 $x_i$ 對應的係數 $w_i$ 為零，該特徵對 $\\hat{y}$ 的變動沒有影響。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 資料與任務已經被定義妥善\n",
    "\n",
    "- 特徵矩陣 $X$\n",
    "- 目標向量 $y$\n",
    "- 係數向量 $w$\n",
    "- 任務：將 $X$ 輸入 $h$ 來預測 $\\hat{y}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 定義評估\n",
    "\n",
    "評估 $h$ 的方法是計算 $y^{(train)}$ 與 $\\hat{y}^{(train)}$ 之間的均方誤差（Mean squared error）。\n",
    "\n",
    "\\begin{equation}\n",
    "\\operatorname*{arg\\,min}_w \\; \\frac{1}{m}\\sum_{i}^{m}{(y^{(train)}_i - \\hat{y_i}^{(train)})^2}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 寫為向量運算的外觀\n",
    "\n",
    "\\begin{equation}\n",
    "\\operatorname*{arg\\,min}_w \\; \\frac{1}{m} \\parallel {y^{(train)}_i - X^{(train)}w \\parallel^2}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 將均方誤差表達為一個基於係數向量 $w$ 的損失函數 $J(w)$\n",
    "\n",
    "\\begin{equation}\n",
    "J(w) = \\frac{1}{m} \\parallel {y^{(train)}_i - X^{(train)}w \\parallel^2}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 整理一下 $J(w)$ 的外觀\n",
    "\n",
    "為了書寫方便，我們省略訓練資料的註記$(train)$。\n",
    "\n",
    "\\begin{align}\n",
    "J(w) &= \\frac{1}{m}(Xw - y)^T(Xw - y) \\\\\n",
    "&= \\frac{1}{m}(w^TX^T - y^T)(Xw - y) \\\\\n",
    "&= \\frac{1}{m}(w^TX^TXw - w^TX^Ty - y^TXw + y^Ty) \\\\\n",
    "&= \\frac{1}{m}(w^TX^TXw - (Xw)^Ty - y^TXw + y^Ty) \\\\\n",
    "&= \\frac{1}{m}(w^TX^TXw - 2(Xw)^Ty + y^Ty)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 求解 $J(w)$ 斜率為零的位置 $w^*$\n",
    "\n",
    "\\begin{gather}\n",
    "\\frac{\\partial}{\\partial w} J(w) = 0 \\\\\n",
    "2X^TXw - 2X^Ty = 0 \\\\\n",
    "X^TXw = X^Ty \\\\\n",
    "w^* = (X^TX)^{-1}X^Ty\n",
    "\\end{gather}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## $w^*$ 求解稱為「正規方程」\n",
    "\n",
    "\\begin{equation}\n",
    "w^* = (X^{(train)T}X^{(train)})^{-1}X^{(train)T}y^{(train)}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 自行定義正規方程類別 NormalEquation\n",
    "\n",
    "```python\n",
    "class NormalEquation:\n",
    "    \"\"\"\n",
    "    This class defines the Normal equation for linear regression.\n",
    "    Args:\n",
    "        fit_intercept (bool): Whether to add intercept for this model.\n",
    "    \"\"\"\n",
    "    def __init__(self, fit_intercept=True):\n",
    "        self._fit_intercept = fit_intercept\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```python\n",
    "    def fit(self, X_train, y_train):\n",
    "        \"\"\"\n",
    "        This function uses Normal equation to solve for weights of this model.\n",
    "        Args:\n",
    "            X_train (ndarray): 2d-array for feature matrix of training data.\n",
    "            y_train (ndarray): 1d-array for target vector of training data.\n",
    "        \"\"\"\n",
    "        self._X_train = X_train.copy()\n",
    "        self._y_train = y_train.copy()\n",
    "        m = self._X_train.shape[0]\n",
    "        if self._fit_intercept:\n",
    "            X0 = np.ones((m, 1), dtype=float)\n",
    "            self._X_train = np.concatenate([X0, self._X_train], axis=1)\n",
    "        X_train_T = np.transpose(self._X_train)\n",
    "        left_matrix = np.dot(X_train_T, self._X_train)\n",
    "        right_matrix = np.dot(X_train_T, self._y_train)\n",
    "        left_matrix_inv = np.linalg.inv(left_matrix)\n",
    "        w = np.dot(left_matrix_inv, right_matrix)\n",
    "        w_ravel = w.ravel().copy()\n",
    "        self._w = w\n",
    "        self.intercept_ = w_ravel[0]\n",
    "        self.coef_ = w_ravel[1:]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```python\n",
    "    def predict(self, X_test):\n",
    "        \"\"\"\n",
    "        This function returns predicted values with weights of this model.\n",
    "        Args:\n",
    "            X_test (ndarray): 2d-array for feature matrix of test data.\n",
    "        \"\"\"\n",
    "        self._X_test = X_test.copy()\n",
    "        m = self._X_test.shape[0]\n",
    "        if self._fit_intercept:\n",
    "            X0 = np.ones((m, 1), dtype=float)\n",
    "            self._X_test = np.concatenate([X0, self._X_test], axis=1)\n",
    "        y_pred = np.dot(self._X_test, self._w)\n",
    "        return y_pred\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "h_sklearn = LinearRegression()\n",
    "h_sklearn.fit(X_train, y_train)\n",
    "h_ne = NormalEquation()\n",
    "h_ne.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-95.14864145823769\n",
      "[97.25416437]\n",
      "-95.1486414580504\n",
      "[97.25416437]\n"
     ]
    }
   ],
   "source": [
    "print(h_sklearn.intercept_) # 截距項\n",
    "print(h_sklearn.coef_)      # 係數項\n",
    "print(h_ne.intercept_)      # 截距項\n",
    "print(h_ne.coef_)           # 係數項"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 95.46952071,  95.46952071,  92.55189578, 107.14002044,\n",
       "        97.414604  ])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 預測\n",
    "y_hat = h_sklearn.predict(X_valid)\n",
    "y_hat[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 95.46952071,  95.46952071,  92.55189578, 107.14002044,\n",
       "        97.414604  ])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 預測\n",
    "y_hat = h_ne.predict(X_valid)\n",
    "y_hat[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 重點統整\n",
    "\n",
    "- 如何決定 `X_train` 與 `y_train` 之間的關聯 $w$：正規方程（Normal equation）\n",
    "- 將 $h(X; w)$ 寫作成向量矩陣相乘形式，為 $w_0$ 補上 $x_0=1$\n",
    "- 求解 $J(w)$ 斜率為零的位置 $\\frac{\\partial}{\\partial w} J(w) = 0$\n",
    "- 正規方程求解 $w^* = (X^TX)^{-1}X^Ty$"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "ML from Scratch",
   "language": "python",
   "name": "mlfromscratch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
